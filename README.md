Задача 1
------------
Описание проекта
------------
* С помощью данного проекта происходит обращение к json файлу (не стал подключать google api, очень много времени займет и выхлоп небольшой)
* json файл парсится, раскидывается информация по справочникам, а потом и данные в головную таблицу
* Описание БД в скриптах sql
* Оркестратор не использовал, потому что нету на локальной машине его

Архитектура проекта
------------
    ├── README.md                     <- Файл README с подробным описанием проекта
    ├── pyproject.toml                <- Версия интерпретатора python и задействованные библиотеки в проекте
    ├── poetry.lock                   <- Зависимости библиотек
    ├── src                           <- Код проекта
    │   └── user_tracking_logs
    │       │
    │       ├── db_connectors         <- Подключение к БД
    │       │
    │       ├── db_writer             <- Модуль для записи данных в БД
    │       │
    │       ├── logs_settongs         <- Установка логгера
    │       └── main                  <- Основной скрипт
    │
    ├── files                         <- json файл
    │
    ├── database_scripts              <- Скрипты sql
    │
    └── authorization_config          <- Конфиги для авторизации в БД

Задача 2
------------
* Не стал писать код, потому что нету под рукой кластера для его проверки и отладки
* Использовал бы кластер Hadoop с установленным PySpark од YARN, орекстратор Oozie или Airflow
* Написал бы bash скрипт с необходимыми переменными для работы PySpark и Python (путь и версии например) и дальнейшим запуском spark-submit
* Логирование реализовал бы с помощью библиотеки logging как в прошлом проекте
* Данные забирал бы c помощью org.apache.dsext.spark.datasource.rest.RestDataSource
* Обработка в spark и дальнейшая загрузка через com.mongodb.spark.sql.connector.MongoTableProvider в MongoDB
* Выбрал MongoDB, потому что рецепты в исходной базе данных удобно хранить в документоориентированной БД